---
# =============================================================================
# SPARK PLAYBOOK - Install and configure Apache Spark with GraphX
# =============================================================================

- name: Configure Apache Spark
  hosts: spark
  gather_facts: true
  become: true
  vars_files:
    - ../variables/main.yml
  
  tasks:
    # -------------------------------------------------------------------------
    # Download and Install Spark
    # -------------------------------------------------------------------------
    # -------------------------------------------------------------------------
    # Download and Install Spark
    # -------------------------------------------------------------------------
    - name: Check if Spark is already installed
      stat:
        path: "{{ spark_home }}/bin/spark-submit"
      register: spark_installed
      tags: [spark, install]

    - name: Remove existing partial Spark directory
      file:
        path: "{{ spark_home }}"
        state: absent
      when: not spark_installed.stat.exists
      tags: [spark, install]

    - name: Download Apache Spark
      get_url:
        url: "{{ spark_download_url }}"
        dest: /tmp/spark.tgz
        mode: '0644'
        timeout: 600
      when: not spark_installed.stat.exists
      tags: [spark, install]

    - name: Extract Spark
      unarchive:
        src: /tmp/spark.tgz
        dest: /opt
        remote_src: yes
      when: not spark_installed.stat.exists
      tags: [spark, install]

    - name: Rename Spark directory
      command: mv /opt/spark-{{ spark_version }}-bin-hadoop{{ spark_hadoop_version }} {{ spark_home }}
      when: not spark_installed.stat.exists
      tags: [spark, install]

    - name: Clean up download
      file:
        path: /tmp/spark.tgz
        state: absent
      tags: [spark, install]

    # -------------------------------------------------------------------------
    # Configure Spark Environment
    # -------------------------------------------------------------------------
    - name: Create Spark environment configuration
      template:
        src: ../roles/spark/templates/spark-env.sh.j2
        dest: "{{ spark_home }}/conf/spark-env.sh"
        mode: '0755'
      notify: Restart Spark
      tags: [spark, config]

    - name: Create Spark defaults configuration
      template:
        src: ../roles/spark/templates/spark-defaults.conf.j2
        dest: "{{ spark_home }}/conf/spark-defaults.conf"
        mode: '0644'
      notify: Restart Spark
      tags: [spark, config]

    - name: Create log4j properties
      template:
        src: ../roles/spark/templates/log4j2.properties.j2
        dest: "{{ spark_home }}/conf/log4j2.properties"
        mode: '0644'
      tags: [spark, config]

    - name: Set Spark environment variables
      lineinfile:
        path: /etc/profile.d/spark.sh
        line: "{{ item }}"
        create: yes
        mode: '0644'
      loop:
        - "export SPARK_HOME={{ spark_home }}"
        - "export PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH"
      tags: [spark, config]

    # -------------------------------------------------------------------------
    # Download Additional Libraries
    # -------------------------------------------------------------------------
    - name: Create jars directory
      file:
        path: "{{ spark_home }}/jars"
        state: directory
        mode: '0755'
      tags: [spark, libraries]

    - name: Download AWS Hadoop library
      get_url:
        url: "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar"
        dest: "{{ spark_home }}/jars/hadoop-aws-3.3.4.jar"
        mode: '0644'
      tags: [spark, libraries]

    - name: Download AWS SDK bundle
      get_url:
        url: "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar"
        dest: "{{ spark_home }}/jars/aws-java-sdk-bundle-1.12.262.jar"
        mode: '0644'
      tags: [spark, libraries]

    - name: Download Neo4j Spark Connector
      get_url:
        url: "https://repo1.maven.org/maven2/org/neo4j/neo4j-connector-apache-spark_2.12/5.2.0_for_spark_3_{{ spark_version | regex_replace('\\.\\d+$', '') }}/neo4j-connector-apache-spark_2.12-5.2.0_for_spark_3_{{ spark_version | regex_replace('\\.\\d+$', '') }}.jar"
        dest: "{{ spark_home }}/jars/neo4j-spark-connector.jar"
        mode: '0644'
      ignore_errors: true  # Version might not match exactly
      tags: [spark, libraries]

    # -------------------------------------------------------------------------
    # Create Systemd Services
    # -------------------------------------------------------------------------
    - name: Create Spark Master systemd service
      template:
        src: ../roles/spark/templates/spark-master.service.j2
        dest: /etc/systemd/system/spark-master.service
        mode: '0644'
      notify: Restart Spark Master
      tags: [spark, service]

    - name: Create Spark Worker systemd service
      template:
        src: ../roles/spark/templates/spark-worker.service.j2
        dest: /etc/systemd/system/spark-worker.service
        mode: '0644'
      notify: Restart Spark Worker
      tags: [spark, service]

    - name: Reload systemd daemon
      systemd:
        daemon_reload: yes
      tags: [spark, service]

    - name: Enable and start Spark Master
      systemd:
        name: spark-master
        enabled: yes
        state: started
      tags: [spark, service]

    - name: Enable and start Spark Worker
      systemd:
        name: spark-worker
        enabled: yes
        state: started
      tags: [spark, service]

    # -------------------------------------------------------------------------
    # Create directories for data
    # -------------------------------------------------------------------------
    - name: Create Spark work directory
      file:
        path: /data/spark
        state: directory
        mode: '0755'
      tags: [spark, directories]

    - name: Create Spark logs directory
      file:
        path: /var/log/spark
        state: directory
        mode: '0755'
      tags: [spark, directories]

    # -------------------------------------------------------------------------
    # Health Check
    # -------------------------------------------------------------------------
    - name: Wait for Spark Master to start
      wait_for:
        port: "{{ spark_master_port }}"
        delay: 10
        timeout: 120
      tags: [spark, verify]

    - name: Wait for Spark Master Web UI
      wait_for:
        port: "{{ spark_master_webui_port }}"
        delay: 5
        timeout: 60
      tags: [spark, verify]

    - name: Print Spark setup completion
      debug:
        msg: |
          âœ… Spark installation completed!
          Master URL: spark://{{ ansible_host }}:{{ spark_master_port }}
          Web UI: http://{{ ansible_host }}:{{ spark_master_webui_port }}
      tags: [spark, verify]

  handlers:
    - name: Restart Spark
      systemd:
        name: "{{ item }}"
        state: restarted
      loop:
        - spark-master
        - spark-worker

    - name: Restart Spark Master
      systemd:
        name: spark-master
        state: restarted

    - name: Restart Spark Worker
      systemd:
        name: spark-worker
        state: restarted
