# Ansible - Graph Analytics Infrastructure

This Ansible project configures the EC2 instances for the Graph Analytics platform.

## ğŸ“ Directory Structure

```
ansible/
â”œâ”€â”€ ansible.cfg              # Ansible configuration
â”œâ”€â”€ run.sh                   # Easy runner script (use from WSL)
â”œâ”€â”€ inventory/
â”‚   â”œâ”€â”€ aws_ec2.yml          # Dynamic inventory (AUTO-DISCOVERS IPs!)
â”‚   â””â”€â”€ hosts.yml            # Static inventory (fallback)
â”œâ”€â”€ playbooks/
â”‚   â”œâ”€â”€ site.yml             # Master playbook (runs all 4)
â”‚   â”œâ”€â”€ zeppelin.yml         # Zeppelin + Spark client + Data prep
â”‚   â”œâ”€â”€ spark.yml            # Spark Master/Worker + GraphX
â”‚   â”œâ”€â”€ giraph.yml           # Hadoop (HDFS/YARN) + Giraph
â”‚   â””â”€â”€ neo4j.yml            # Neo4j + APOC + Data import
â”œâ”€â”€ roles/
â”‚   â”œâ”€â”€ zeppelin/templates/  # Zeppelin config templates
â”‚   â”œâ”€â”€ spark/templates/     # Spark config templates
â”‚   â”œâ”€â”€ giraph/templates/    # Hadoop/Giraph config templates
â”‚   â””â”€â”€ neo4j/templates/     # Neo4j config templates
â”œâ”€â”€ variables/
â”‚   â”œâ”€â”€ main.yml             # Global variables
â”‚   â””â”€â”€ secrets.yml          # YOUR SECRETS (create this, gitignored!)
â””â”€â”€ logs/                    # Ansible logs
```

## ğŸš€ Quick Start

### Prerequisites

1. **Install Ansible (in WSL)**
   ```bash
   sudo apt update && sudo apt install ansible python3-pip
   pip3 install boto3 botocore
   ansible-galaxy collection install amazon.aws
   ```

2. **Configure AWS Credentials**
   ```bash
   aws configure
   ```

3. **Create secrets.yml**
   ```bash
   cp variables/secrets.yml.template variables/secrets.yml
   nano variables/secrets.yml
   ```

### Running Playbooks

```bash
cd ansible

# Run ALL playbooks (complete setup)
./run.sh playbooks/site.yml

# Run individual playbooks
./run.sh playbooks/zeppelin.yml    # Zeppelin + Spark client
./run.sh playbooks/spark.yml       # Spark Master/Worker
./run.sh playbooks/giraph.yml      # Hadoop + Giraph
./run.sh playbooks/neo4j.yml       # Neo4j database

# Optional: Download dataset (after zeppelin.yml)
./run.sh playbooks/zeppelin.yml --tags data

# Optional: Import dataset to Neo4j (after neo4j.yml and data)
./run.sh playbooks/neo4j.yml --tags import
```

## ğŸ“‹ What Each Playbook Installs

### 1. zeppelin.yml (Analytics Hub)
- âœ… Common packages + Java 11
- âœ… Apache Zeppelin 0.11.2
- âœ… Apache Spark client (for driver)
- âœ… Python packages (neo4j, pandas, numpy, matplotlib, networkx)
- âœ… Data download from Kaggle (optional --tags data)

### 2. spark.yml (Compute Cluster)
- âœ… Common packages + Java 11
- âœ… Apache Spark 3.5.0 Master + Worker
- âœ… GraphX (included in Spark)
- âœ… AWS S3 libraries

### 3. giraph.yml (Graph Processing)
- âœ… Apache Hadoop 3.3.6 (HDFS + YARN)
- âœ… Apache Giraph 1.2.0
- âœ… Sample graph data + PageRank script

### 4. neo4j.yml (Graph Database)
- âœ… Common packages + Java 17
- âœ… Neo4j Community 5.x
- âœ… APOC plugin
- âœ… Data import from S3 (optional --tags import)

## ğŸ” Secrets Management

**âš ï¸ IMPORTANT:** Never commit `secrets.yml` to git!

```yaml
# variables/secrets.yml
---
neo4j_admin_password: "your-secure-password"
kaggle_api_token: "your-kaggle-token"
```

## ğŸ§ª Recommended Run Order

For a fresh deployment:

```bash
# 1. Run all base playbooks
./run.sh playbooks/site.yml

# 2. Download dataset
./run.sh playbooks/zeppelin.yml --tags data

# 3. Import to Neo4j
./run.sh playbooks/neo4j.yml --tags import

# 4. Open Zeppelin and test!
# http://<zeppelin-public-ip>:8080
```

## ğŸ“Š Service URLs (After Deployment)

| Service | URL | Notes |
|---------|-----|-------|
| Zeppelin | http://\<zeppelin-ip\>:8080 | Public access |
| Spark Master UI | http://\<spark-ip\>:8081 | Internal only |
| HDFS NameNode UI | http://\<spark-ip\>:9870 | Internal only |
| YARN ResourceManager | http://\<spark-ip\>:8088 | Internal only |
| Neo4j Browser | http://\<neo4j-ip\>:7474 | Internal only |
| Neo4j Bolt | bolt://\<neo4j-ip\>:7687 | Internal only |

## ğŸ› Troubleshooting

### Timeout errors
All large downloads use `async` - just wait or re-run the playbook.

### SSH issues
```bash
# Test SSH directly
ssh -i ~/hajar-project-key.pem ubuntu@<ip>
```

### Service not starting
```bash
# Check logs on the instance
sudo journalctl -u zeppelin -f
sudo journalctl -u spark-master -f
sudo journalctl -u neo4j -f
```

### Neo4j Spark Connector version mismatch
The connector is loaded via `--packages` in Zeppelin to ensure version consistency. Do NOT install it manually on the Spark server.
